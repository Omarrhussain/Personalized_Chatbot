{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13398093,"sourceType":"datasetVersion","datasetId":8502266}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain langchain_community google-generativeai sentence-transformers faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:05:23.032881Z","iopub.execute_input":"2025-11-07T15:05:23.033153Z","iopub.status.idle":"2025-11-07T15:05:26.697634Z","shell.execute_reply.started":"2025-11-07T15:05:23.033131Z","shell.execute_reply":"2025-11-07T15:05:26.696746Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.schema import Document\nimport google.generativeai as genai\nfrom google.api_core import exceptions\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:05:26.698571Z","iopub.execute_input":"2025-11-07T15:05:26.698780Z","iopub.status.idle":"2025-11-07T15:05:31.009475Z","shell.execute_reply.started":"2025-11-07T15:05:26.698761Z","shell.execute_reply":"2025-11-07T15:05:31.008853Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Get your Gemini API key from: https://makersuite.google.com/app/apikey\nGEMINI_API_KEY = \"AIzaSyDd5u96ISj9Z5wW7ueAjDmTUQDgM3yIT44\"\n\n# Configure Gemini\ngenai.configure(api_key=GEMINI_API_KEY)\n\n# Test Gemini connection\ntry:\n    model = genai.GenerativeModel('gemini-2.5-flash')\n    response = model.generate_content(\"Hello, are you working?\")\n    print(\"âœ… Gemini API connected successfully!\")\n    print(f\"Test response: {response.text}\")\nexcept Exception as e:\n    print(f\"âŒ Gemini API connection failed: {e}\")\n    print(\"Please check your API key and make sure it's valid\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:05:31.011335Z","iopub.execute_input":"2025-11-07T15:05:31.011789Z","iopub.status.idle":"2025-11-07T15:05:34.657035Z","shell.execute_reply.started":"2025-11-07T15:05:31.011770Z","shell.execute_reply":"2025-11-07T15:05:34.656242Z"}},"outputs":[{"name":"stdout","text":"âœ… Gemini API connected successfully!\nTest response: Hello! Yes, you could say I am. As an AI, I'm always \"on\" and ready to assist by processing information and generating responses.\n\nHow can I help you today?\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"try:\n    # Try different possible file paths\n    try:\n        df = pd.read_csv('/kaggle/input/chatbot-dataset/cleaned_conversations.csv')\n    except:\n        df = pd.read_csv('/kaggle/input/chatbot-dataset/cleaned_conversations.csv')\n    print(f\"âœ… Dataset loaded! Size: {len(df)} conversations\")\n    print(f\"Dataset columns: {df.columns.tolist()}\")\nexcept Exception as e:\n    print(f\"âŒ Error loading dataset: {e}\")\n    print(\"Creating sample dataset for testing...\")\n    # Create sample data if file not found\n    df = pd.DataFrame({\n        'input': [\n            'What is artificial intelligence?',\n            'Explain machine learning',\n            'What is deep learning?',\n            'How does neural network work?',\n            'What is natural language processing?'\n        ],\n        'response': [\n            'Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems.',\n            'Machine learning is a subset of AI that enables computers to learn and make decisions from data without being explicitly programmed.',\n            'Deep learning is a type of machine learning that uses neural networks with multiple layers to analyze various factors in data.',\n            'Neural networks are computing systems inspired by biological neural networks that learn to perform tasks by considering examples.',\n            'Natural language processing is a branch of AI that helps computers understand, interpret and manipulate human language.'\n        ]\n    })\n    print(\"âœ… Sample dataset created for testing\")\n\n# Combine input and response for chunking\ndf['combined_text'] = df['input'] + \" \" + df['response']\nprint(\"Sample combined text:\")\nprint(df['combined_text'].iloc[0][:200] + \"...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:05:34.657780Z","iopub.execute_input":"2025-11-07T15:05:34.657992Z","iopub.status.idle":"2025-11-07T15:05:34.883329Z","shell.execute_reply.started":"2025-11-07T15:05:34.657976Z","shell.execute_reply":"2025-11-07T15:05:34.882715Z"}},"outputs":[{"name":"stdout","text":"âœ… Dataset loaded! Size: 121838 conversations\nDataset columns: ['input', 'response']\nSample combined text:\nhi getting ready cheetah chasing stay shape must fast hunting one favorite hobby...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Convert DataFrame to LangChain Documents\ndocuments = []\nfor idx, row in df.iterrows():\n    doc = Document(\n        page_content=row['combined_text'],\n        metadata={\n            'input': row['input'],\n            'response': row['response'],\n            'source': 'conversation_data',\n            'id': idx\n        }\n    )\n    documents.append(doc)\n\nprint(f\"âœ… Created {len(documents)} documents\")\n\n# Initialize text splitter for chunking\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # Larger chunks since we're not fine-tuning\n    chunk_overlap=100,\n    length_function=len,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n\n# Split documents into chunks\nchunks = text_splitter.split_documents(documents)\nprint(f\"âœ… Created {len(chunks)} chunks from {len(documents)} documents\")\nprint(f\"Sample chunk: {chunks[0].page_content[:200]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:05:34.884095Z","iopub.execute_input":"2025-11-07T15:05:34.884356Z","iopub.status.idle":"2025-11-07T15:05:43.337121Z","shell.execute_reply.started":"2025-11-07T15:05:34.884338Z","shell.execute_reply":"2025-11-07T15:05:43.336291Z"}},"outputs":[{"name":"stdout","text":"âœ… Created 121838 documents\nâœ… Created 121838 chunks from 121838 documents\nSample chunk: hi getting ready cheetah chasing stay shape must fast hunting one favorite hobby...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Initialize embeddings model\nprint(\"ğŸ”„ Loading embedding model...\")\nembedding_model = HuggingFaceEmbeddings(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n    model_kwargs={'device': 'cpu'}\n)\n\n# Create FAISS vector store\nprint(\"ğŸ”„ Creating FAISS vector database...\")\nvector_db = FAISS.from_documents(chunks, embedding_model)\n\n# Save the vector database\nvector_db.save_local(\"vector_db/gemini_rag\")\nprint(\"âœ… Vector database created and saved!\")\n\n# Test retrieval\nquery = \"What is artificial intelligence?\"\nsimilar_docs = vector_db.similarity_search(query, k=2)\nprint(f\"\\nğŸ” Retrieval test for: '{query}'\")\nfor i, doc in enumerate(similar_docs):\n    print(f\"Result {i+1}: {doc.page_content[:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:05:43.338030Z","iopub.execute_input":"2025-11-07T15:05:43.338824Z","iopub.status.idle":"2025-11-07T15:10:08.054131Z","shell.execute_reply.started":"2025-11-07T15:05:43.338803Z","shell.execute_reply":"2025-11-07T15:10:08.053353Z"}},"outputs":[{"name":"stdout","text":"ğŸ”„ Loading embedding model...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_137/25980592.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embedding_model = HuggingFaceEmbeddings(\n2025-11-07 15:05:46.210581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762527946.233070     137 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762527946.240054     137 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"ğŸ”„ Creating FAISS vector database...\nâœ… Vector database created and saved!\n\nğŸ” Retrieval test for: 'What is artificial intelligence?'\nResult 1: ai well time ever tried video game mortal combat...\nResult 2: really robotics...\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class GeminiRAGSystem:\n    def __init__(self, vector_db, gemini_api_key):\n        self.vector_db = vector_db\n        self.retriever = vector_db.as_retriever(search_kwargs={\"k\": 4})\n        genai.configure(api_key=gemini_api_key)\n        self.model = genai.GenerativeModel('gemini-2.5-flash')\n        \n    def get_context(self, question):\n        \"\"\"Retrieve relevant context from vector database\"\"\"\n        docs = self.retriever.get_relevant_documents(question)\n        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n        return context, docs\n    \n    def ask_question(self, question, conversation_history=[]):\n        \"\"\"Ask question with RAG context\"\"\"\n        # Get relevant context\n        context, source_docs = self.get_context(question)\n        \n        # Build conversation history\n        history_text = \"\"\n        if conversation_history:\n            history_text = \"\\nPrevious conversation:\\n\"\n            for i, (q, a) in enumerate(conversation_history[-3:]):  # Last 3 exchanges\n                history_text += f\"Q: {q}\\nA: {a}\\n\"\n        \n        # Create enhanced prompt\n        prompt = f\"\"\"Based on the following context and conversation history, please answer the question.\n\nContext Information:\n{context}\n{history_text}\nCurrent Question: {question}\n\nPlease provide a helpful and accurate answer based on the context provided. If the context doesn't contain enough information, you can use your general knowledge but please indicate this.\"\"\"\n\n        try:\n            # Generate response using Gemini\n            response = self.model.generate_content(prompt)\n            answer = response.text\n            \n            return {\n                'question': question,\n                'answer': answer,\n                'sources': source_docs,\n                'context_used': context[:500] + \"...\" if len(context) > 500 else context\n            }\n            \n        except exceptions.InvalidArgument as e:\n            return {\n                'question': question,\n                'answer': f\"Error: Invalid API key or configuration. Please check your Gemini API key.\",\n                'sources': [],\n                'context_used': \"\"\n            }\n        except Exception as e:\n            return {\n                'question': question,\n                'answer': f\"Error generating response: {str(e)}\",\n                'sources': [],\n                'context_used': \"\"\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:10:08.054884Z","iopub.execute_input":"2025-11-07T15:10:08.055136Z","iopub.status.idle":"2025-11-07T15:10:08.063292Z","shell.execute_reply.started":"2025-11-07T15:10:08.055107Z","shell.execute_reply":"2025-11-07T15:10:08.062473Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"ğŸ”„ Initializing Gemini RAG System...\")\ngemini_rag = GeminiRAGSystem(vector_db, GEMINI_API_KEY)\nprint(\"âœ… Gemini RAG System ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:10:08.064180Z","iopub.execute_input":"2025-11-07T15:10:08.064851Z","iopub.status.idle":"2025-11-07T15:10:08.082019Z","shell.execute_reply.started":"2025-11-07T15:10:08.064831Z","shell.execute_reply":"2025-11-07T15:10:08.081457Z"}},"outputs":[{"name":"stdout","text":"ğŸ”„ Initializing Gemini RAG System...\nâœ… Gemini RAG System ready!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def test_gemini_rag(question, conversation_history=[]):\n    print(f\"ğŸ¤” Question: {question}\")\n    print(\"â³ Generating response...\")\n    \n    result = gemini_rag.ask_question(question, conversation_history)\n    \n    print(f\"ğŸ¤– Gemini Answer: {result['answer']}\")\n    print(f\"\\nğŸ“š Context used: {result['context_used']}\")\n    print(f\"\\nğŸ” Sources retrieved ({len(result['sources'])}):\")\n    for i, doc in enumerate(result['sources']):\n        print(f\"Source {i+1}: {doc.page_content[:150]}...\")\n        print(f\"   Metadata: {doc.metadata}\\n\")\n    \n    return result\n\n# Test with single question\nprint(\"ğŸ§ª Testing Gemini RAG System...\")\nprint(\"=\" * 80)\n\ntest_question = \"What is artificial intelligence?\"\nresult1 = test_gemini_rag(test_question)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:10:08.082753Z","iopub.execute_input":"2025-11-07T15:10:08.082949Z","iopub.status.idle":"2025-11-07T15:10:12.864192Z","shell.execute_reply.started":"2025-11-07T15:10:08.082935Z","shell.execute_reply":"2025-11-07T15:10:12.863552Z"}},"outputs":[{"name":"stdout","text":"ğŸ§ª Testing Gemini RAG System...\n================================================================================\nğŸ¤” Question: What is artificial intelligence?\nâ³ Generating response...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_137/1144091497.py:10: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  docs = self.retriever.get_relevant_documents(question)\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¤– Gemini Answer: Based on the provided context, \"ai\" is used as an abbreviation. The conversation also refers to \"human like robot\" in relation to Isaac Asimov's book series, suggesting that artificial intelligence could involve creating robots that mimic human capabilities or appearance.\n\nHowever, the context doesn't offer a direct definition of what artificial intelligence is.\n\n**Using general knowledge:** Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. It encompasses various fields like machine learning, natural language processing, computer vision, and robotics, aiming to enable machines to perform tasks that typically require human intellect, such as learning, problem-solving, and understanding.\n\nğŸ“š Context used: ai well time ever tried video game mortal combat\n\nreally robotics\n\nmeant robot lol hey cheating five word\n\nheard science based yes book series isaac asimov human like robot\n\nğŸ” Sources retrieved (4):\nSource 1: ai well time ever tried video game mortal combat...\n   Metadata: {'input': 'ai well time', 'response': 'ever tried video game mortal combat', 'source': 'conversation_data', 'id': 103738}\n\nSource 2: really robotics...\n   Metadata: {'input': 'really', 'response': 'robotics', 'source': 'conversation_data', 'id': 58640}\n\nSource 3: meant robot lol hey cheating five word...\n   Metadata: {'input': 'meant robot lol', 'response': 'hey cheating five word', 'source': 'conversation_data', 'id': 110688}\n\nSource 4: heard science based yes book series isaac asimov human like robot...\n   Metadata: {'input': 'heard science based', 'response': 'yes book series isaac asimov human like robot', 'source': 'conversation_data', 'id': 762}\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"=\" * 80)\n\n# Test with follow-up question (conversational)\nfollow_up = \"Can you explain machine learning too?\"\nconversation_history = [(test_question, result1['answer'])]\nresult2 = test_gemini_rag(follow_up, conversation_history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:10:12.864899Z","iopub.execute_input":"2025-11-07T15:10:12.865087Z","iopub.status.idle":"2025-11-07T15:10:18.392515Z","shell.execute_reply.started":"2025-11-07T15:10:12.865070Z","shell.execute_reply":"2025-11-07T15:10:18.391688Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ¤” Question: Can you explain machine learning too?\nâ³ Generating response...\nğŸ¤– Gemini Answer: Based on the provided context and conversation history, there isn't enough information to explain \"machine learning\" directly. While the context mentions \"learning curve\" and \"love learning computer creature,\" these don't define the technical concept of machine learning. The previous answer to \"What is artificial intelligence?\" did mention machine learning as a field encompassed by AI, but did not define it.\n\n**Using general knowledge:**\nMachine learning (ML) is a subfield of artificial intelligence (AI) that empowers computer systems to learn from data without being explicitly programmed. Instead of following pre-defined instructions for every possible scenario, ML algorithms are designed to analyze vast amounts of data, recognize patterns, and make predictions or decisions based on what they've learned.\n\nIn essence, it allows machines to \"learn\" from experience, much like humans do. The more data an ML model processes, the more it can refine its understanding and improve its performance on a given task. This learning process often involves training models with data to identify relationships and then applying those learned patterns to new, unseen data.\n\nCommon applications of machine learning include:\n*   **Recommendation systems:** Suggesting products, movies, or music based on your past preferences.\n*   **Spam detection:** Identifying and filtering unwanted emails.\n*   **Image and speech recognition:** Enabling devices to understand spoken commands or recognize faces.\n*   **Medical diagnosis:** Assisting doctors in identifying diseases from medical images or patient data.\n\nğŸ“š Context used: guess understand thing like\n\ncool irritating time bit learning curve\n\nthank god science right right love learning computer creature\n\nhelp understand something anything need\n\nğŸ” Sources retrieved (4):\nSource 1: guess understand thing like...\n   Metadata: {'input': 'guess understand', 'response': 'thing like', 'source': 'conversation_data', 'id': 86547}\n\nSource 2: cool irritating time bit learning curve...\n   Metadata: {'input': 'cool irritating time', 'response': 'bit learning curve', 'source': 'conversation_data', 'id': 64885}\n\nSource 3: thank god science right right love learning computer creature...\n   Metadata: {'input': 'thank god science right', 'response': 'right love learning computer creature', 'source': 'conversation_data', 'id': 23238}\n\nSource 4: help understand something anything need...\n   Metadata: {'input': 'help understand', 'response': 'something anything need', 'source': 'conversation_data', 'id': 91888}\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def interactive_chat():\n    \"\"\"Start an interactive chat with the Gemini RAG system\"\"\"\n    print(\"\\nğŸ’¬ Starting Interactive Chat Mode!\")\n    print(\"Type 'quit' to exit, 'history' to see conversation history\")\n    print(\"-\" * 50)\n    \n    conversation_history = []\n    \n    while True:\n        user_input = input(\"\\nYou: \").strip()\n        \n        if user_input.lower() == 'quit':\n            print(\"Goodbye! ğŸ‘‹\")\n            break\n        elif user_input.lower() == 'history':\n            print(\"\\nğŸ“œ Conversation History:\")\n            for i, (q, a) in enumerate(conversation_history):\n                print(f\"{i+1}. Q: {q}\")\n                print(f\"   A: {a[:100]}...\")\n            continue\n        elif not user_input:\n            continue\n            \n        print(\"â³ Thinking...\")\n        result = gemini_rag.ask_question(user_input, conversation_history)\n        \n        print(f\"\\nğŸ¤– Gemini: {result['answer']}\")\n        \n        # Add to conversation history\n        conversation_history.append((user_input, result['answer']))\n        \n        # Show sources if available\n        if result['sources']:\n            print(f\"\\nğŸ“š Sources used: {len(result['sources'])} relevant documents\")\n\nprint(\"âœ… Interactive chat function ready!\")\nprint(\"To start chatting, uncomment and run: interactive_chat()\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:10:18.393421Z","iopub.execute_input":"2025-11-07T15:10:18.393757Z","iopub.status.idle":"2025-11-07T15:10:18.400734Z","shell.execute_reply.started":"2025-11-07T15:10:18.393736Z","shell.execute_reply":"2025-11-07T15:10:18.400085Z"}},"outputs":[{"name":"stdout","text":"âœ… Interactive chat function ready!\nTo start chatting, uncomment and run: interactive_chat()\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def batch_test_questions(questions):\n    \"\"\"Test multiple questions at once\"\"\"\n    print(\"ğŸ§ª Batch Testing Questions...\")\n    print(\"=\" * 60)\n    \n    results = []\n    for i, question in enumerate(questions, 1):\n        print(f\"\\n{i}. Question: {question}\")\n        result = gemini_rag.ask_question(question)\n        print(f\"   Answer: {result['answer'][:150]}...\")\n        results.append(result)\n    \n    return results\n\n# Test multiple questions\ntest_questions = [\n    \"What is artificial intelligence?\",\n    \"Explain machine learning in simple terms\",\n    \"How does deep learning work?\",\n    \"What are the applications of AI?\"\n]\n\nbatch_results = batch_test_questions(test_questions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:10:18.403078Z","iopub.execute_input":"2025-11-07T15:10:18.403282Z","iopub.status.idle":"2025-11-07T15:10:39.555778Z","shell.execute_reply.started":"2025-11-07T15:10:18.403267Z","shell.execute_reply":"2025-11-07T15:10:39.554941Z"}},"outputs":[{"name":"stdout","text":"ğŸ§ª Batch Testing Questions...\n============================================================\n\n1. Question: What is artificial intelligence?\n   Answer: The provided context doesn't directly define artificial intelligence.\n\nBased on general knowledge, artificial intelligence (AI) is the simulation of h...\n\n2. Question: Explain machine learning in simple terms\n   Answer: Based on the context provided, which only indicates a desire to understand (\"need help help understand\", \"want know trying understand understand wrote...\n\n3. Question: How does deep learning work?\n   Answer: Based on the context provided, there isn't enough information to explain how deep learning works. The conversation mentions \"deep cool\" and \"really de...\n\n4. Question: What are the applications of AI?\n   Answer: Based on the provided context, the information regarding applications of AI is very limited. The conversation mentions \"robotics\" and gives a single, ...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(\"ğŸ“¦ Saving vector database for future use...\")\nshutil.make_archive('gemini_rag_vector_db', 'zip', 'vector_db')\nprint(\"âœ… Vector database saved as 'gemini_rag_vector_db.zip'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:10:39.556548Z","iopub.execute_input":"2025-11-07T15:10:39.556815Z","iopub.status.idle":"2025-11-07T15:10:49.855134Z","shell.execute_reply.started":"2025-11-07T15:10:39.556795Z","shell.execute_reply":"2025-11-07T15:10:49.854203Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¦ Saving vector database for future use...\nâœ… Vector database saved as 'gemini_rag_vector_db.zip'\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def load_gemini_rag_system(vector_db_path, gemini_api_key):\n    \"\"\"Load a saved Gemini RAG system\"\"\"\n    print(\"ğŸ”„ Loading saved Gemini RAG system...\")\n    \n    # Load embeddings\n    embedding_model = HuggingFaceEmbeddings(\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n    )\n    \n    # Load vector database\n    vector_db = FAISS.load_local(vector_db_path, embedding_model)\n    \n    # Create Gemini RAG system\n    gemini_rag = GeminiRAGSystem(vector_db, gemini_api_key)\n    print(\"âœ… Gemini RAG system loaded successfully!\")\n    return gemini_rag\n\nprint(\"âœ… Utility function created!\")\nprint(\"To load system later, use: load_gemini_rag_system('vector_db/gemini_rag', GEMINI_API_KEY)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:10:49.856008Z","iopub.execute_input":"2025-11-07T15:10:49.856271Z","iopub.status.idle":"2025-11-07T15:10:49.861472Z","shell.execute_reply.started":"2025-11-07T15:10:49.856244Z","shell.execute_reply":"2025-11-07T15:10:49.860790Z"}},"outputs":[{"name":"stdout","text":"âœ… Utility function created!\nTo load system later, use: load_gemini_rag_system('vector_db/gemini_rag', GEMINI_API_KEY)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(\"\\nğŸ“Š Performance Comparison\")\nprint(\"=\" * 50)\nprint(\"âœ… Advantages of Gemini API approach:\")\nprint(\"   â€¢ Instant setup (no training required)\")\nprint(\"   â€¢ No GPU needed during inference\")\nprint(\"   â€¢ Much faster responses\")\nprint(\"   â€¢ Access to Google's latest model\")\nprint(\"   â€¢ Cost-effective for most use cases\")\nprint(\"   â€¢ Easy to update knowledge\")\nprint(\"   â€¢ Built-in safety features\")\n\nprint(\"\\nğŸ’¡ Cost Note: Gemini Pro API costs ~$0.000125 per 1K characters\")\nprint(\"   (Very affordable for most applications)\")\n\nprint(\"\\nğŸ‰ Your Gemini RAG System is Ready!\")\nprint(\"\\nğŸ“ Files created:\")\nprint(\"   â€¢ Vector database: 'vector_db/gemini_rag/'\")\nprint(\"   â€¢ Downloadable zip: 'gemini_rag_vector_db.zip'\")\n\nprint(\"\\nğŸ”§ Quick Usage Example:\")\nprint(\"\"\"\n# Ask a question\nresult = gemini_rag.ask_question(\"What is AI?\")\nprint(result['answer'])\n\n# Conversational chat\nhistory = []\nresult1 = gemini_rag.ask_question(\"First question\", history)\nhistory.append((\"First question\", result1['answer']))\nresult2 = gemini_rag.ask_question(\"Follow-up question\", history)\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:10:49.862000Z","iopub.execute_input":"2025-11-07T15:10:49.862252Z","iopub.status.idle":"2025-11-07T15:10:49.878414Z","shell.execute_reply.started":"2025-11-07T15:10:49.862234Z","shell.execute_reply":"2025-11-07T15:10:49.877876Z"}},"outputs":[{"name":"stdout","text":"\nğŸ“Š Performance Comparison\n==================================================\nâœ… Advantages of Gemini API approach:\n   â€¢ Instant setup (no training required)\n   â€¢ No GPU needed during inference\n   â€¢ Much faster responses\n   â€¢ Access to Google's latest model\n   â€¢ Cost-effective for most use cases\n   â€¢ Easy to update knowledge\n   â€¢ Built-in safety features\n\nğŸ’¡ Cost Note: Gemini Pro API costs ~$0.000125 per 1K characters\n   (Very affordable for most applications)\n\nğŸ‰ Your Gemini RAG System is Ready!\n\nğŸ“ Files created:\n   â€¢ Vector database: 'vector_db/gemini_rag/'\n   â€¢ Downloadable zip: 'gemini_rag_vector_db.zip'\n\nğŸ”§ Quick Usage Example:\n\n# Ask a question\nresult = gemini_rag.ask_question(\"What is AI?\")\nprint(result['answer'])\n\n# Conversational chat\nhistory = []\nresult1 = gemini_rag.ask_question(\"First question\", history)\nhistory.append((\"First question\", result1['answer']))\nresult2 = gemini_rag.ask_question(\"Follow-up question\", history)\n\n","output_type":"stream"}],"execution_count":15}]}