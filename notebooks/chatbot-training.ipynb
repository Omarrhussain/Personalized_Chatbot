{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:05:23.033153Z",
     "iopub.status.busy": "2025-11-07T15:05:23.032881Z",
     "iopub.status.idle": "2025-11-07T15:05:26.697634Z",
     "shell.execute_reply": "2025-11-07T15:05:26.696746Z",
     "shell.execute_reply.started": "2025-11-07T15:05:23.033131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain_community google-generativeai sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:05:26.698780Z",
     "iopub.status.busy": "2025-11-07T15:05:26.698571Z",
     "iopub.status.idle": "2025-11-07T15:05:31.009475Z",
     "shell.execute_reply": "2025-11-07T15:05:31.008853Z",
     "shell.execute_reply.started": "2025-11-07T15:05:26.698761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import google.generativeai as genai\n",
    "from google.api_core import exceptions\n",
    "import shutil\n",
    "from config.api_keys import GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:05:31.011789Z",
     "iopub.status.busy": "2025-11-07T15:05:31.011335Z",
     "iopub.status.idle": "2025-11-07T15:05:34.657035Z",
     "shell.execute_reply": "2025-11-07T15:05:34.656242Z",
     "shell.execute_reply.started": "2025-11-07T15:05:31.011770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini API connected successfully!\n",
      "Test response: Hello! Yes, you could say I am. As an AI, I'm always \"on\" and ready to assist by processing information and generating responses.\n",
      "\n",
      "How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Get your Gemini API key from: https://makersuite.google.com/app/apikey\n",
    "GEMINI_API_KEY = GEMINI_API_KEY\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Test Gemini connection\n",
    "try:\n",
    "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "    response = model.generate_content(\"Hello, are you working?\")\n",
    "    print(\"‚úÖ Gemini API connected successfully!\")\n",
    "    print(f\"Test response: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Gemini API connection failed: {e}\")\n",
    "    print(\"Please check your API key and make sure it's valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:05:34.657992Z",
     "iopub.status.busy": "2025-11-07T15:05:34.657780Z",
     "iopub.status.idle": "2025-11-07T15:05:34.883329Z",
     "shell.execute_reply": "2025-11-07T15:05:34.882715Z",
     "shell.execute_reply.started": "2025-11-07T15:05:34.657976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded! Size: 121838 conversations\n",
      "Dataset columns: ['input', 'response']\n",
      "Sample combined text:\n",
      "hi getting ready cheetah chasing stay shape must fast hunting one favorite hobby...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Try different possible file paths\n",
    "    try:\n",
    "        df = pd.read_csv('/kaggle/input/chatbot-dataset/cleaned_conversations.csv')\n",
    "    except:\n",
    "        df = pd.read_csv('/kaggle/input/chatbot-dataset/cleaned_conversations.csv')\n",
    "    print(f\"‚úÖ Dataset loaded! Size: {len(df)} conversations\")\n",
    "    print(f\"Dataset columns: {df.columns.tolist()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(\"Creating sample dataset for testing...\")\n",
    "    # Create sample data if file not found\n",
    "    df = pd.DataFrame({\n",
    "        'input': [\n",
    "            'What is artificial intelligence?',\n",
    "            'Explain machine learning',\n",
    "            'What is deep learning?',\n",
    "            'How does neural network work?',\n",
    "            'What is natural language processing?'\n",
    "        ],\n",
    "        'response': [\n",
    "            'Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems.',\n",
    "            'Machine learning is a subset of AI that enables computers to learn and make decisions from data without being explicitly programmed.',\n",
    "            'Deep learning is a type of machine learning that uses neural networks with multiple layers to analyze various factors in data.',\n",
    "            'Neural networks are computing systems inspired by biological neural networks that learn to perform tasks by considering examples.',\n",
    "            'Natural language processing is a branch of AI that helps computers understand, interpret and manipulate human language.'\n",
    "        ]\n",
    "    })\n",
    "    print(\"‚úÖ Sample dataset created for testing\")\n",
    "\n",
    "# Combine input and response for chunking\n",
    "df['combined_text'] = df['input'] + \" \" + df['response']\n",
    "print(\"Sample combined text:\")\n",
    "print(df['combined_text'].iloc[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:05:34.884356Z",
     "iopub.status.busy": "2025-11-07T15:05:34.884095Z",
     "iopub.status.idle": "2025-11-07T15:05:43.337121Z",
     "shell.execute_reply": "2025-11-07T15:05:43.336291Z",
     "shell.execute_reply.started": "2025-11-07T15:05:34.884338Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 121838 documents\n",
      "‚úÖ Created 121838 chunks from 121838 documents\n",
      "Sample chunk: hi getting ready cheetah chasing stay shape must fast hunting one favorite hobby...\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to LangChain Documents\n",
    "documents = []\n",
    "for idx, row in df.iterrows():\n",
    "    doc = Document(\n",
    "        page_content=row['combined_text'],\n",
    "        metadata={\n",
    "            'input': row['input'],\n",
    "            'response': row['response'],\n",
    "            'source': 'conversation_data',\n",
    "            'id': idx\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents)} documents\")\n",
    "\n",
    "# Initialize text splitter for chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Larger chunks since we're not fine-tuning\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"Sample chunk: {chunks[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:05:43.338824Z",
     "iopub.status.busy": "2025-11-07T15:05:43.338030Z",
     "iopub.status.idle": "2025-11-07T15:10:08.054131Z",
     "shell.execute_reply": "2025-11-07T15:10:08.053353Z",
     "shell.execute_reply.started": "2025-11-07T15:05:43.338803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137/25980592.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "2025-11-07 15:05:46.210581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762527946.233070     137 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762527946.240054     137 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating FAISS vector database...\n",
      "‚úÖ Vector database created and saved!\n",
      "\n",
      "üîç Retrieval test for: 'What is artificial intelligence?'\n",
      "Result 1: ai well time ever tried video game mortal combat...\n",
      "Result 2: really robotics...\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings model\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# Create FAISS vector store\n",
    "print(\"üîÑ Creating FAISS vector database...\")\n",
    "vector_db = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Save the vector database\n",
    "vector_db.save_local(\"vector_db/gemini_rag\")\n",
    "print(\"‚úÖ Vector database created and saved!\")\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is artificial intelligence?\"\n",
    "similar_docs = vector_db.similarity_search(query, k=2)\n",
    "print(f\"\\nüîç Retrieval test for: '{query}'\")\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"Result {i+1}: {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:10:08.055136Z",
     "iopub.status.busy": "2025-11-07T15:10:08.054884Z",
     "iopub.status.idle": "2025-11-07T15:10:08.063292Z",
     "shell.execute_reply": "2025-11-07T15:10:08.062473Z",
     "shell.execute_reply.started": "2025-11-07T15:10:08.055107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GeminiRAGSystem:\n",
    "    def __init__(self, vector_db, gemini_api_key):\n",
    "        self.vector_db = vector_db\n",
    "        self.retriever = vector_db.as_retriever(search_kwargs={\"k\": 4})\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "        \n",
    "    def get_context(self, question):\n",
    "        \"\"\"Retrieve relevant context from vector database\"\"\"\n",
    "        docs = self.retriever.get_relevant_documents(question)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        return context, docs\n",
    "    \n",
    "    def ask_question(self, question, conversation_history=[]):\n",
    "        \"\"\"Ask question with RAG context\"\"\"\n",
    "        # Get relevant context\n",
    "        context, source_docs = self.get_context(question)\n",
    "        \n",
    "        # Build conversation history\n",
    "        history_text = \"\"\n",
    "        if conversation_history:\n",
    "            history_text = \"\\nPrevious conversation:\\n\"\n",
    "            for i, (q, a) in enumerate(conversation_history[-3:]):  # Last 3 exchanges\n",
    "                history_text += f\"Q: {q}\\nA: {a}\\n\"\n",
    "        \n",
    "        # Create enhanced prompt\n",
    "        prompt = f\"\"\"Based on the following context and conversation history, please answer the question.\n",
    "\n",
    "Context Information:\n",
    "{context}\n",
    "{history_text}\n",
    "Current Question: {question}\n",
    "\n",
    "Please provide a helpful and accurate answer based on the context provided. If the context doesn't contain enough information, you can use your general knowledge but please indicate this.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Generate response using Gemini\n",
    "            response = self.model.generate_content(prompt)\n",
    "            answer = response.text\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'sources': source_docs,\n",
    "                'context_used': context[:500] + \"...\" if len(context) > 500 else context\n",
    "            }\n",
    "            \n",
    "        except exceptions.InvalidArgument as e:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': f\"Error: Invalid API key or configuration. Please check your Gemini API key.\",\n",
    "                'sources': [],\n",
    "                'context_used': \"\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': f\"Error generating response: {str(e)}\",\n",
    "                'sources': [],\n",
    "                'context_used': \"\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:10:08.064851Z",
     "iopub.status.busy": "2025-11-07T15:10:08.064180Z",
     "iopub.status.idle": "2025-11-07T15:10:08.082019Z",
     "shell.execute_reply": "2025-11-07T15:10:08.081457Z",
     "shell.execute_reply.started": "2025-11-07T15:10:08.064831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing Gemini RAG System...\n",
      "‚úÖ Gemini RAG System ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Initializing Gemini RAG System...\")\n",
    "gemini_rag = GeminiRAGSystem(vector_db, GEMINI_API_KEY)\n",
    "print(\"‚úÖ Gemini RAG System ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:10:08.082949Z",
     "iopub.status.busy": "2025-11-07T15:10:08.082753Z",
     "iopub.status.idle": "2025-11-07T15:10:12.864192Z",
     "shell.execute_reply": "2025-11-07T15:10:12.863552Z",
     "shell.execute_reply.started": "2025-11-07T15:10:08.082935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Gemini RAG System...\n",
      "================================================================================\n",
      "ü§î Question: What is artificial intelligence?\n",
      "‚è≥ Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137/1144091497.py:10: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = self.retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Gemini Answer: Based on the provided context, \"ai\" is used as an abbreviation. The conversation also refers to \"human like robot\" in relation to Isaac Asimov's book series, suggesting that artificial intelligence could involve creating robots that mimic human capabilities or appearance.\n",
      "\n",
      "However, the context doesn't offer a direct definition of what artificial intelligence is.\n",
      "\n",
      "**Using general knowledge:** Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. It encompasses various fields like machine learning, natural language processing, computer vision, and robotics, aiming to enable machines to perform tasks that typically require human intellect, such as learning, problem-solving, and understanding.\n",
      "\n",
      "üìö Context used: ai well time ever tried video game mortal combat\n",
      "\n",
      "really robotics\n",
      "\n",
      "meant robot lol hey cheating five word\n",
      "\n",
      "heard science based yes book series isaac asimov human like robot\n",
      "\n",
      "üîç Sources retrieved (4):\n",
      "Source 1: ai well time ever tried video game mortal combat...\n",
      "   Metadata: {'input': 'ai well time', 'response': 'ever tried video game mortal combat', 'source': 'conversation_data', 'id': 103738}\n",
      "\n",
      "Source 2: really robotics...\n",
      "   Metadata: {'input': 'really', 'response': 'robotics', 'source': 'conversation_data', 'id': 58640}\n",
      "\n",
      "Source 3: meant robot lol hey cheating five word...\n",
      "   Metadata: {'input': 'meant robot lol', 'response': 'hey cheating five word', 'source': 'conversation_data', 'id': 110688}\n",
      "\n",
      "Source 4: heard science based yes book series isaac asimov human like robot...\n",
      "   Metadata: {'input': 'heard science based', 'response': 'yes book series isaac asimov human like robot', 'source': 'conversation_data', 'id': 762}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_gemini_rag(question, conversation_history=[]):\n",
    "    print(f\"ü§î Question: {question}\")\n",
    "    print(\"‚è≥ Generating response...\")\n",
    "    \n",
    "    result = gemini_rag.ask_question(question, conversation_history)\n",
    "    \n",
    "    print(f\"ü§ñ Gemini Answer: {result['answer']}\")\n",
    "    print(f\"\\nüìö Context used: {result['context_used']}\")\n",
    "    print(f\"\\nüîç Sources retrieved ({len(result['sources'])}):\")\n",
    "    for i, doc in enumerate(result['sources']):\n",
    "        print(f\"Source {i+1}: {doc.page_content[:150]}...\")\n",
    "        print(f\"   Metadata: {doc.metadata}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with single question\n",
    "print(\"üß™ Testing Gemini RAG System...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_question = \"What is artificial intelligence?\"\n",
    "result1 = test_gemini_rag(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:10:12.865087Z",
     "iopub.status.busy": "2025-11-07T15:10:12.864899Z",
     "iopub.status.idle": "2025-11-07T15:10:18.392515Z",
     "shell.execute_reply": "2025-11-07T15:10:18.391688Z",
     "shell.execute_reply.started": "2025-11-07T15:10:12.865070Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ü§î Question: Can you explain machine learning too?\n",
      "‚è≥ Generating response...\n",
      "ü§ñ Gemini Answer: Based on the provided context and conversation history, there isn't enough information to explain \"machine learning\" directly. While the context mentions \"learning curve\" and \"love learning computer creature,\" these don't define the technical concept of machine learning. The previous answer to \"What is artificial intelligence?\" did mention machine learning as a field encompassed by AI, but did not define it.\n",
      "\n",
      "**Using general knowledge:**\n",
      "Machine learning (ML) is a subfield of artificial intelligence (AI) that empowers computer systems to learn from data without being explicitly programmed. Instead of following pre-defined instructions for every possible scenario, ML algorithms are designed to analyze vast amounts of data, recognize patterns, and make predictions or decisions based on what they've learned.\n",
      "\n",
      "In essence, it allows machines to \"learn\" from experience, much like humans do. The more data an ML model processes, the more it can refine its understanding and improve its performance on a given task. This learning process often involves training models with data to identify relationships and then applying those learned patterns to new, unseen data.\n",
      "\n",
      "Common applications of machine learning include:\n",
      "*   **Recommendation systems:** Suggesting products, movies, or music based on your past preferences.\n",
      "*   **Spam detection:** Identifying and filtering unwanted emails.\n",
      "*   **Image and speech recognition:** Enabling devices to understand spoken commands or recognize faces.\n",
      "*   **Medical diagnosis:** Assisting doctors in identifying diseases from medical images or patient data.\n",
      "\n",
      "üìö Context used: guess understand thing like\n",
      "\n",
      "cool irritating time bit learning curve\n",
      "\n",
      "thank god science right right love learning computer creature\n",
      "\n",
      "help understand something anything need\n",
      "\n",
      "üîç Sources retrieved (4):\n",
      "Source 1: guess understand thing like...\n",
      "   Metadata: {'input': 'guess understand', 'response': 'thing like', 'source': 'conversation_data', 'id': 86547}\n",
      "\n",
      "Source 2: cool irritating time bit learning curve...\n",
      "   Metadata: {'input': 'cool irritating time', 'response': 'bit learning curve', 'source': 'conversation_data', 'id': 64885}\n",
      "\n",
      "Source 3: thank god science right right love learning computer creature...\n",
      "   Metadata: {'input': 'thank god science right', 'response': 'right love learning computer creature', 'source': 'conversation_data', 'id': 23238}\n",
      "\n",
      "Source 4: help understand something anything need...\n",
      "   Metadata: {'input': 'help understand', 'response': 'something anything need', 'source': 'conversation_data', 'id': 91888}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "\n",
    "# Test with follow-up question (conversational)\n",
    "follow_up = \"Can you explain machine learning too?\"\n",
    "conversation_history = [(test_question, result1['answer'])]\n",
    "result2 = test_gemini_rag(follow_up, conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:10:18.393757Z",
     "iopub.status.busy": "2025-11-07T15:10:18.393421Z",
     "iopub.status.idle": "2025-11-07T15:10:18.400734Z",
     "shell.execute_reply": "2025-11-07T15:10:18.400085Z",
     "shell.execute_reply.started": "2025-11-07T15:10:18.393736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Interactive chat function ready!\n",
      "To start chatting, uncomment and run: interactive_chat()\n"
     ]
    }
   ],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"Start an interactive chat with the Gemini RAG system\"\"\"\n",
    "    print(\"\\nüí¨ Starting Interactive Chat Mode!\")\n",
    "    print(\"Type 'quit' to exit, 'history' to see conversation history\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Goodbye! üëã\")\n",
    "            break\n",
    "        elif user_input.lower() == 'history':\n",
    "            print(\"\\nüìú Conversation History:\")\n",
    "            for i, (q, a) in enumerate(conversation_history):\n",
    "                print(f\"{i+1}. Q: {q}\")\n",
    "                print(f\"   A: {a[:100]}...\")\n",
    "            continue\n",
    "        elif not user_input:\n",
    "            continue\n",
    "            \n",
    "        print(\"‚è≥ Thinking...\")\n",
    "        result = gemini_rag.ask_question(user_input, conversation_history)\n",
    "        \n",
    "        print(f\"\\nü§ñ Gemini: {result['answer']}\")\n",
    "        \n",
    "        # Add to conversation history\n",
    "        conversation_history.append((user_input, result['answer']))\n",
    "        \n",
    "        # Show sources if available\n",
    "        if result['sources']:\n",
    "            print(f\"\\nüìö Sources used: {len(result['sources'])} relevant documents\")\n",
    "\n",
    "print(\"‚úÖ Interactive chat function ready!\")\n",
    "print(\"To start chatting, uncomment and run: interactive_chat()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:10:18.403282Z",
     "iopub.status.busy": "2025-11-07T15:10:18.403078Z",
     "iopub.status.idle": "2025-11-07T15:10:39.555778Z",
     "shell.execute_reply": "2025-11-07T15:10:39.554941Z",
     "shell.execute_reply.started": "2025-11-07T15:10:18.403267Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Batch Testing Questions...\n",
      "============================================================\n",
      "\n",
      "1. Question: What is artificial intelligence?\n",
      "   Answer: The provided context doesn't directly define artificial intelligence.\n",
      "\n",
      "Based on general knowledge, artificial intelligence (AI) is the simulation of h...\n",
      "\n",
      "2. Question: Explain machine learning in simple terms\n",
      "   Answer: Based on the context provided, which only indicates a desire to understand (\"need help help understand\", \"want know trying understand understand wrote...\n",
      "\n",
      "3. Question: How does deep learning work?\n",
      "   Answer: Based on the context provided, there isn't enough information to explain how deep learning works. The conversation mentions \"deep cool\" and \"really de...\n",
      "\n",
      "4. Question: What are the applications of AI?\n",
      "   Answer: Based on the provided context, the information regarding applications of AI is very limited. The conversation mentions \"robotics\" and gives a single, ...\n"
     ]
    }
   ],
   "source": [
    "def batch_test_questions(questions):\n",
    "    \"\"\"Test multiple questions at once\"\"\"\n",
    "    print(\"üß™ Batch Testing Questions...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n{i}. Question: {question}\")\n",
    "        result = gemini_rag.ask_question(question)\n",
    "        print(f\"   Answer: {result['answer'][:150]}...\")\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test multiple questions\n",
    "test_questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain machine learning in simple terms\",\n",
    "    \"How does deep learning work?\",\n",
    "    \"What are the applications of AI?\"\n",
    "]\n",
    "\n",
    "batch_results = batch_test_questions(test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:10:39.556815Z",
     "iopub.status.busy": "2025-11-07T15:10:39.556548Z",
     "iopub.status.idle": "2025-11-07T15:10:49.855134Z",
     "shell.execute_reply": "2025-11-07T15:10:49.854203Z",
     "shell.execute_reply.started": "2025-11-07T15:10:39.556795Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Saving vector database for future use...\n",
      "‚úÖ Vector database saved as 'gemini_rag_vector_db.zip'\n"
     ]
    }
   ],
   "source": [
    "print(\"üì¶ Saving vector database for future use...\")\n",
    "shutil.make_archive('gemini_rag_vector_db', 'zip', 'vector_db')\n",
    "print(\"‚úÖ Vector database saved as 'gemini_rag_vector_db.zip'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:10:49.856271Z",
     "iopub.status.busy": "2025-11-07T15:10:49.856008Z",
     "iopub.status.idle": "2025-11-07T15:10:49.861472Z",
     "shell.execute_reply": "2025-11-07T15:10:49.860790Z",
     "shell.execute_reply.started": "2025-11-07T15:10:49.856244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Utility function created!\n",
      "To load system later, use: load_gemini_rag_system('vector_db/gemini_rag', GEMINI_API_KEY)\n"
     ]
    }
   ],
   "source": [
    "def load_gemini_rag_system(vector_db_path, gemini_api_key):\n",
    "    \"\"\"Load a saved Gemini RAG system\"\"\"\n",
    "    print(\"üîÑ Loading saved Gemini RAG system...\")\n",
    "    \n",
    "    # Load embeddings\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    \n",
    "    # Load vector database\n",
    "    vector_db = FAISS.load_local(vector_db_path, embedding_model)\n",
    "    \n",
    "    # Create Gemini RAG system\n",
    "    gemini_rag = GeminiRAGSystem(vector_db, gemini_api_key)\n",
    "    print(\"‚úÖ Gemini RAG system loaded successfully!\")\n",
    "    return gemini_rag\n",
    "\n",
    "print(\"‚úÖ Utility function created!\")\n",
    "print(\"To load system later, use: load_gemini_rag_system('vector_db/gemini_rag', GEMINI_API_KEY)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T15:10:49.862252Z",
     "iopub.status.busy": "2025-11-07T15:10:49.862000Z",
     "iopub.status.idle": "2025-11-07T15:10:49.878414Z",
     "shell.execute_reply": "2025-11-07T15:10:49.877876Z",
     "shell.execute_reply.started": "2025-11-07T15:10:49.862234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Performance Comparison\n",
      "==================================================\n",
      "‚úÖ Advantages of Gemini API approach:\n",
      "   ‚Ä¢ Instant setup (no training required)\n",
      "   ‚Ä¢ No GPU needed during inference\n",
      "   ‚Ä¢ Much faster responses\n",
      "   ‚Ä¢ Access to Google's latest model\n",
      "   ‚Ä¢ Cost-effective for most use cases\n",
      "   ‚Ä¢ Easy to update knowledge\n",
      "   ‚Ä¢ Built-in safety features\n",
      "\n",
      "üí° Cost Note: Gemini Pro API costs ~$0.000125 per 1K characters\n",
      "   (Very affordable for most applications)\n",
      "\n",
      "üéâ Your Gemini RAG System is Ready!\n",
      "\n",
      "üìÅ Files created:\n",
      "   ‚Ä¢ Vector database: 'vector_db/gemini_rag/'\n",
      "   ‚Ä¢ Downloadable zip: 'gemini_rag_vector_db.zip'\n",
      "\n",
      "üîß Quick Usage Example:\n",
      "\n",
      "# Ask a question\n",
      "result = gemini_rag.ask_question(\"What is AI?\")\n",
      "print(result['answer'])\n",
      "\n",
      "# Conversational chat\n",
      "history = []\n",
      "result1 = gemini_rag.ask_question(\"First question\", history)\n",
      "history.append((\"First question\", result1['answer']))\n",
      "result2 = gemini_rag.ask_question(\"Follow-up question\", history)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Performance Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Advantages of Gemini API approach:\")\n",
    "print(\"   ‚Ä¢ Instant setup (no training required)\")\n",
    "print(\"   ‚Ä¢ No GPU needed during inference\")\n",
    "print(\"   ‚Ä¢ Much faster responses\")\n",
    "print(\"   ‚Ä¢ Access to Google's latest model\")\n",
    "print(\"   ‚Ä¢ Cost-effective for most use cases\")\n",
    "print(\"   ‚Ä¢ Easy to update knowledge\")\n",
    "print(\"   ‚Ä¢ Built-in safety features\")\n",
    "\n",
    "print(\"\\nüí° Cost Note: Gemini Pro API costs ~$0.000125 per 1K characters\")\n",
    "print(\"   (Very affordable for most applications)\")\n",
    "\n",
    "print(\"\\nüéâ Your Gemini RAG System is Ready!\")\n",
    "print(\"\\nüìÅ Files created:\")\n",
    "print(\"   ‚Ä¢ Vector database: 'vector_db/gemini_rag/'\")\n",
    "print(\"   ‚Ä¢ Downloadable zip: 'gemini_rag_vector_db.zip'\")\n",
    "\n",
    "print(\"\\nüîß Quick Usage Example:\")\n",
    "print(\"\"\"\n",
    "# Ask a question\n",
    "result = gemini_rag.ask_question(\"What is AI?\")\n",
    "print(result['answer'])\n",
    "\n",
    "# Conversational chat\n",
    "history = []\n",
    "result1 = gemini_rag.ask_question(\"First question\", history)\n",
    "history.append((\"First question\", result1['answer']))\n",
    "result2 = gemini_rag.ask_question(\"Follow-up question\", history)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8502266,
     "sourceId": 13398093,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv (3.9.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
