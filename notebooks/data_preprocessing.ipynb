{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eced03a",
   "metadata": {},
   "source": [
    "## Data Cleaning Strategies Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6707a484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "class PreprocessingExperiments:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.text_col = [col for col in df.columns if any(x in col.lower() for x in ['text', 'message'])][0]\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def experiment_cleaning_levels(self):\n",
    "        \"\"\"Compare different cleaning intensities\"\"\"\n",
    "        sample_texts = self.df[self.text_col].dropna().sample(5).tolist()\n",
    "        \n",
    "        cleaning_strategies = {\n",
    "            'light_clean': self.light_cleaning,\n",
    "            'medium_clean': self.medium_cleaning, \n",
    "            'heavy_clean': self.heavy_cleaning\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for strategy_name, strategy_func in cleaning_strategies.items():\n",
    "            cleaned_texts = [strategy_func(text) for text in sample_texts]\n",
    "            results[strategy_name] = cleaned_texts\n",
    "            \n",
    "            print(f\"\\n=== {strategy_name.upper()} ===\")\n",
    "            for orig, clean in zip(sample_texts, cleaned_texts):\n",
    "                print(f\"Original: {orig[:100]}...\")\n",
    "                print(f\"Cleaned:  {clean[:100]}...\")\n",
    "                print(\"-\" * 50)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def light_cleaning(self, text):\n",
    "        \"\"\"Basic cleaning - keep most information\"\"\"\n",
    "        text = str(text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def medium_cleaning(self, text):\n",
    "        \"\"\"Moderate cleaning for general NLP\"\"\"\n",
    "        text = self.light_cleaning(text)\n",
    "        text = text.lower()\n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text\n",
    "    \n",
    "    def heavy_cleaning(self, text):\n",
    "        \"\"\"Heavy cleaning for focused analysis\"\"\"\n",
    "        text = self.medium_cleaning(text)\n",
    "        # Remove all punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove stopwords\n",
    "        tokens = text.split()\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32952d29",
   "metadata": {},
   "source": [
    "## Tokenization Methods Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1e4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_experiment(self):\n",
    "    \"\"\"Compare different tokenization approaches\"\"\"\n",
    "    sample_text = self.df[self.text_col].dropna().iloc[0]\n",
    "    \n",
    "    tokenization_methods = {\n",
    "        'word_tokenize': nltk.word_tokenize,\n",
    "        'split': lambda x: x.split(),\n",
    "        'regex_tokenize': lambda x: re.findall(r'\\b\\w+\\b', x)\n",
    "    }\n",
    "    \n",
    "    print(\"Original Text:\", sample_text)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    for method_name, method_func in tokenization_methods.items():\n",
    "        tokens = method_func(sample_text)\n",
    "        print(f\"\\n{method_name}:\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token Count: {len(tokens)}\")\n",
    "        print(f\"Unique Tokens: {len(set(tokens))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44c1ce",
   "metadata": {},
   "source": [
    "## Stemming vs Lemmatization Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977597a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_vs_lemmatization(self):\n",
    "    \"\"\"Compare stemming and lemmatization results\"\"\"\n",
    "    sample_words = ['running', 'ran', 'runs', 'better', 'best', 'books', 'booking']\n",
    "    \n",
    "    stemmed = [self.stemmer.stem(word) for word in sample_words]\n",
    "    lemmatized = [self.lemmatizer.lemmatize(word) for word in sample_words]\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Original': sample_words,\n",
    "        'Stemmed': stemmed,\n",
    "        'Lemmatized': lemmatized\n",
    "    })\n",
    "    \n",
    "    print(\"Stemming vs Lemmatization Comparison:\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Impact on vocabulary size\n",
    "    sample_texts = self.df[self.text_col].dropna().head(100)\n",
    "    \n",
    "    original_vocab = set()\n",
    "    stemmed_vocab = set()\n",
    "    lemmatized_vocab = set()\n",
    "    \n",
    "    for text in sample_texts:\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "        original_vocab.update(tokens)\n",
    "        stemmed_vocab.update([self.stemmer.stem(token) for token in tokens])\n",
    "        lemmatized_vocab.update([self.lemmatizer.lemmatize(token) for token in tokens])\n",
    "    \n",
    "    print(f\"\\nVocabulary Size Impact:\")\n",
    "    print(f\"Original: {len(original_vocab)}\")\n",
    "    print(f\"Stemmed: {len(stemmed_vocab)}\")\n",
    "    print(f\"Lemmatized: {len(lemmatized_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5ca46",
   "metadata": {},
   "source": [
    "## Stopword Removal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "706feebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_analysis(self):\n",
    "    \"\"\"Analyze impact of stopword removal\"\"\"\n",
    "    sample_texts = self.df[self.text_col].dropna().head(50)\n",
    "    \n",
    "    # Custom stopwords for personality analysis\n",
    "    personality_stopwords = {'i', 'you', 'we', 'they', 'my', 'your', 'our', 'their'}\n",
    "    extended_stopwords = self.stop_words.union(personality_stopwords)\n",
    "    \n",
    "    strategies = {\n",
    "        'no_removal': set(),\n",
    "        'standard_stopwords': self.stop_words,\n",
    "        'extended_stopwords': extended_stopwords\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for strategy_name, stopword_set in strategies.items():\n",
    "        processed_texts = []\n",
    "        for text in sample_texts:\n",
    "            tokens = word_tokenize(str(text).lower())\n",
    "            if stopword_set:\n",
    "                tokens = [token for token in tokens if token not in stopword_set]\n",
    "            processed_texts.append(' '.join(tokens))\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_words = sum(len(text.split()) for text in processed_texts)\n",
    "        unique_words = len(set(' '.join(processed_texts).split()))\n",
    "        \n",
    "        results[strategy_name] = {\n",
    "            'total_words': total_words,\n",
    "            'unique_words': unique_words,\n",
    "            'avg_words_per_text': total_words / len(sample_texts)\n",
    "        }\n",
    "    \n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(\"Stopword Removal Impact:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac5794",
   "metadata": {},
   "source": [
    "##  Personality-Specific Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97651c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personality_specific_preprocessing(self):\n",
    "    \"\"\"Experiment with personality-aware preprocessing\"\"\"\n",
    "    if 'personality' not in self.df.columns:\n",
    "        return\n",
    "    \n",
    "    # Analyze vocabulary differences between personalities\n",
    "    personalities = self.df['personality'].unique()\n",
    "    \n",
    "    personality_vocab = {}\n",
    "    for personality in personalities:\n",
    "        personality_texts = self.df[self.df['personality'] == personality][self.text_col]\n",
    "        all_words = ' '.join(personality_texts.astype(str)).lower().split()\n",
    "        word_freq = Counter(all_words)\n",
    "        personality_vocab[personality] = word_freq\n",
    "    \n",
    "    # Find personality-specific words\n",
    "    print(\"Personality-Specific Vocabulary Analysis:\")\n",
    "    for personality, vocab in personality_vocab.items():\n",
    "        common_words = set(vocab.keys())\n",
    "        other_words = set()\n",
    "        for other_personality, other_vocab in personality_vocab.items():\n",
    "            if other_personality != personality:\n",
    "                other_words.update(other_vocab.keys())\n",
    "        \n",
    "        unique_words = common_words - other_words\n",
    "        print(f\"\\n{personality} unique words (top 10):\")\n",
    "        unique_word_freq = {word: vocab[word] for word in unique_words if word in vocab}\n",
    "        print(sorted(unique_word_freq.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825ea7f",
   "metadata": {},
   "source": [
    "## Visualization Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b154ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing_visualizations(self):\n",
    "    \"\"\"Create visualizations to compare preprocessing effects\"\"\"\n",
    "    sample_texts = self.df[self.text_col].dropna().head(100)\n",
    "    \n",
    "    # Before and after cleaning word clouds\n",
    "    original_text = ' '.join(sample_texts.astype(str))\n",
    "    cleaned_text = ' '.join([self.medium_cleaning(text) for text in sample_texts])\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Original word cloud\n",
    "    wordcloud_orig = WordCloud(width=600, height=400, background_color='white').generate(original_text)\n",
    "    ax1.imshow(wordcloud_orig, interpolation='bilinear')\n",
    "    ax1.set_title('Original Text Word Cloud')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Cleaned word cloud\n",
    "    wordcloud_clean = WordCloud(width=600, height=400, background_color='white').generate(cleaned_text)\n",
    "    ax2.imshow(wordcloud_clean, interpolation='bilinear')\n",
    "    ax2.set_title('Cleaned Text Word Cloud')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
