{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71025555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Personalized_Chatbot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete - Using fixed evaluation pipeline\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our fixed evaluation classes\n",
    "from evaluation_pipeline import FixedChatbotEvaluator, run_complete_evaluation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Setup complete - Using fixed evaluation pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f781190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Complete Evaluation...\n",
      "ü§ñ CHATBOT EVALUATION PIPELINE\n",
      "============================================================\n",
      "üéØ Using latest checkpoint: D:\\Personalized_Chatbot\\models\\fine-tuned-chatbot\\checkpoint-27414 (step 27414)\n",
      "\n",
      "üéØ EVALUATING FINE-TUNED MODEL...\n",
      "üîÑ Loading model with LoRA adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loading adapter from: D:\\Personalized_Chatbot\\models\\fine-tuned-chatbot\\checkpoint-27414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Personalized_Chatbot\\venv\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "d:\\Personalized_Chatbot\\venv\\lib\\site-packages\\peft\\peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìä Starting evaluation...\n",
      "Evaluating on 25 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [06:25<00:00, 15.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Calculating metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity:   0%|          | 0/25 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "Calculating Perplexity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [01:48<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ EVALUATING BASE MODEL...\n",
      "üîÑ Loading model with LoRA adapter...\n",
      "‚ö†Ô∏è  Using base model only\n",
      "‚úÖ Model loaded successfully!\n",
      "üìä Starting evaluation...\n",
      "Evaluating on 25 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [06:20<00:00, 15.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Calculating metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [02:18<00:00,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìà EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üéØ FINE-TUNED MODEL RESULTS:\n",
      "  bleu_score     : 0.0000\n",
      "  rouge1         : 0.0018\n",
      "  rouge2         : 0.0000\n",
      "  rougeL         : 0.0018\n",
      "  perplexity     : nan\n",
      "\n",
      "üîµ BASE MODEL RESULTS:\n",
      "  bleu_score     : 0.0000\n",
      "  rouge1         : 0.0057\n",
      "  rouge2         : 0.0008\n",
      "  rougeL         : 0.0048\n",
      "  perplexity     : 130960.4079\n",
      "\n",
      "üìä IMPROVEMENT (Fine-tuned vs Base):\n",
      "  ----------------------------------------\n",
      "  bleu_score     : +0.0000 (+0.0%) ‚Üì\n",
      "  rouge1         : -0.0039 (-67.9%) ‚Üì\n",
      "  rouge2         : -0.0008 (-100.0%) ‚Üì\n",
      "  rougeL         : -0.0031 (-62.9%) ‚Üì\n",
      "  perplexity     : +nan (+nan%) ‚Üì\n",
      "\n",
      "============================================================\n",
      "üß™ SAMPLE RESPONSES\n",
      "============================================================\n",
      "\n",
      "Input: cool keep sneezing darn allergy much dust\n",
      "Reference: cool keep sneezing darn allergy much dust\n",
      "Fine-tuned: A specialising in game management and strategy, the Director of Content Development and Media for the Centre for Digital Media and Culture at the University of Edinburgh. He is a Fellow of the National Policy Institute and a member of the Committee for the Advancement of Digital Media at the University of Edinburgh.\n",
      "Base: do you have any suggestions for the \"shoe\" of the fish?\n",
      "A: no\n",
      "(No, I don't think you could have created this on the wiki without a lot of discussion or discussion).\n",
      "A: I have no idea how to edit the game.\n",
      "A: I have no idea if there are any differences between the two.\n",
      "A: I will try to write a review for the game.\n",
      "A: I have no idea what the fish look like without having\n",
      "--------------------------------------------------\n",
      "\n",
      "Input: good filming putting makeup youtube channel\n",
      "Reference: good filming putting makeup youtube channel\n",
      "Fine-tuned: Matt Latham\n",
      "Director: Peter Latham, MSP\n",
      "Professor: John Danton\n",
      "Professor: John Danton\n",
      "University Of Sheffield\n",
      "Professor: Paul Boselli\n",
      "Director: Peter Latham, MSP\n",
      "Professor: Paul Boselli\n",
      "Professor: John Danton\n",
      "MID: 1st Assistant Professor\n",
      "Professor: Paul Boselli\n",
      "Director: Peter Latham, MSP\n",
      "Professor: John Danton\n",
      "Professor: John Danton\n",
      "Professor: Paul Boselli\n",
      "Base: Dr. James MacKenzie, Associate Professor of Public Health at the University of British Columbia\n",
      "A spokesperson for the Food and Drug Administration said a new study shows that Americans eat more meat than any other nation on earth and that the number of people who eat meat is rising.\n",
      "The study, which was funded by the U.S.-based Center for Environmental Health Research, was published in The American Journal of Medical Genetics in March.\n",
      "The study, which was based on the U.S.-\n",
      "--------------------------------------------------\n",
      "\n",
      "Input: needed paramedic threw arm big game\n",
      "Reference: needed paramedic threw arm big game\n",
      "Fine-tuned: John D. R\n",
      "Base: no\n",
      "Bachelor of Arts: sociology\n",
      "Bachelor of Arts: sociology\n",
      "Bachelor of Arts: sociology\n",
      "Bachelor of Arts: sociology\n",
      "Bachelor of Arts: sociology\n",
      "Bachelor of Arts: sociology\n",
      "Bachelor of Arts: sociology\n",
      "Bachelor of Arts: sociology\n",
      "Bachelor of Arts: sociology\n",
      "Bachelor of Arts: sociology\n",
      "Bachelor of Arts: sociology\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting Complete Evaluation...\")\n",
    "results_ft, results_base = run_complete_evaluation()\n",
    "\n",
    "if results_ft is None:\n",
    "    print(\"\\n‚ö†Ô∏è  No fine-tuned model found. Running base model evaluation only...\")\n",
    "    from evaluation_pipeline import FixedChatbotEvaluator\n",
    "    evaluator = FixedChatbotEvaluator(adapter_path=None)\n",
    "    results_base, _, _ = evaluator.evaluate_on_dataset(sample_size=25)\n",
    "    results_ft = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d82823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading models for qualitative analysis...\n",
      "‚úÖ Using checkpoint: ../models/fine-tuned-chatbot\\checkpoint-27414\n",
      "üîÑ Loading model with LoRA adapter...\n",
      "‚ö†Ô∏è  Using base model only\n",
      "‚ö†Ô∏è  Using base model only\n",
      "‚úÖ Model loaded successfully!\n",
      "üîÑ Loading model with LoRA adapter...\n",
      "‚úÖ Model loaded successfully!\n",
      "üîÑ Loading model with LoRA adapter...\n",
      "‚úÖ Loading adapter from: ../models/fine-tuned-chatbot\\checkpoint-27414\n",
      "‚úÖ Loading adapter from: ../models/fine-tuned-chatbot\\checkpoint-27414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Personalized_Chatbot\\venv\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "d:\\Personalized_Chatbot\\venv\\lib\\site-packages\\peft\\peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "\n",
      "üß™ ENHANCED QUALITATIVE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìù Input: hi got done online job\n",
      "‚úÖ Reference: like tired working day wek\n",
      "üîµ Base Model: A lot of them were starting to get into it. So I don't know if they had a lot of experience working on the video game but it's not that hard for me.\n",
      "What do you think of the recent changes to how the game is set up?\n",
      "What's the impact it has on the game?\n",
      "I think that's what I think the game has on the whole. I think that the game is still a huge part of the game. I think that there are a\n",
      "üîµ Base Model: A lot of them were starting to get into it. So I don't know if they had a lot of experience working on the video game but it's not that hard for me.\n",
      "What do you think of the recent changes to how the game is set up?\n",
      "What's the impact it has on the game?\n",
      "I think that's what I think the game has on the whole. I think that the game is still a huge part of the game. I think that there are a\n",
      "üéØ Fine-tuned: hi got done online job: hello got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìù Input: love swim play sport\n",
      "‚úÖ Reference: go swimming everyday plan weekend\n",
      "üéØ Fine-tuned: hi got done online job: hello got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online job: hi got done online\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìù Input: love swim play sport\n",
      "‚úÖ Reference: go swimming everyday plan weekend\n",
      "üîµ Base Model: Scott C. Lecar, Senior Vice President\n",
      "üéØ Fine-tuned: J.D. Stilwell\n",
      "Deputy:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìù Input: sound fun especially cooler weather coming\n",
      "‚úÖ Reference: yeah also like sing play pencil\n",
      "üîµ Base Model: a little bit more complicated\n",
      "Powers\n",
      "In the meantime, check out the blog post I wrote for the blog post:\n",
      "Advertisements\n",
      "üéØ Fine-tuned: It's an awesome experience and I'm excited to see what the future holds for me.\n",
      "I'm currently working on getting the new audio system to my PC and mobile. I'm also working on making the 3D audio system for PC and mobile.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìù Input: started working home internet heck thing\n",
      "‚úÖ Reference: work real estate restaurant weekend\n",
      "üîµ Base Model: I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn, NY, so I'm from Brooklyn\n",
      "üéØ Fine-tuned: \"I don't want to be like 'I don't want to be like 'I don't want to be like 'I don't want to be like 'I don't want to be like 'I don't want to be like 'I don't want to be like 'I don't want to be like 'I don't want to be like 'I don't want to be like 'I don't want to be like 'I don't want to be like 'I don't\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìù Input: parent died car crash like build modelplanes\n",
      "‚úÖ Reference: stink man lost use leg car wreck\n",
      "üîµ Base Model: teacher reported crash close\n",
      "üéØ Fine-tuned: teacher killed in car crash like build modelplanes\n",
      "Universities: child killed in car crash like build modelplanes\n",
      "School: child died in car crash like build modelplanes\n",
      "City: girl fatally shot in car crash like build modelplanes\n",
      "School: child killed in car crash like build modelplanes\n",
      "School: child killed in car crash like build modelplanes\n",
      "School: child killed in car crash like build modelplanes\n",
      "School: child killed in car crash like build modelplanes\n",
      "School: child\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def enhanced_qualitative_analysis(num_samples=5):\n",
    "    # Load data\n",
    "    try:\n",
    "        df = pd.read_csv('D:\\Personalized_Chatbot\\data\\processed\\cleaned_conversations.csv')\n",
    "        samples = df.sample(min(num_samples, len(df)))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize evaluators\n",
    "    print(\"üîÑ Loading models for qualitative analysis...\")\n",
    "    \n",
    "    # Find latest checkpoint for fine-tuned model\n",
    "    adapter_path = None\n",
    "    model_dir = \"../models/fine-tuned-chatbot\"\n",
    "    if os.path.exists(model_dir):\n",
    "        checkpoints = []\n",
    "        for item in os.listdir(model_dir):\n",
    "            if item.startswith(\"checkpoint-\"):\n",
    "                try:\n",
    "                    step = int(item.split(\"-\")[1])\n",
    "                    checkpoints.append((step, os.path.join(model_dir, item)))\n",
    "                except:\n",
    "                    continue\n",
    "        if checkpoints:\n",
    "            latest_step, adapter_path = max(checkpoints, key=lambda x: x[0])\n",
    "            print(f\"‚úÖ Using checkpoint: {adapter_path}\")\n",
    "    \n",
    "    # Create evaluators\n",
    "    evaluator_base = FixedChatbotEvaluator(adapter_path=None)\n",
    "    if adapter_path:\n",
    "        evaluator_ft = FixedChatbotEvaluator(adapter_path=adapter_path)\n",
    "    else:\n",
    "        evaluator_ft = None\n",
    "        print(\"‚ö†Ô∏è  No fine-tuned model available for comparison\")\n",
    "    \n",
    "    print(\"\\nüß™ ENHANCED QUALITATIVE ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for idx, row in samples.iterrows():\n",
    "        input_text = row['input']\n",
    "        reference = row['response']\n",
    "        \n",
    "        print(f\"\\nüìù Input: {input_text}\")\n",
    "        print(f\"‚úÖ Reference: {reference}\")\n",
    "        \n",
    "        # Base model response\n",
    "        base_response = evaluator_base.generate_response(input_text)\n",
    "        print(f\"üîµ Base Model: {base_response}\")\n",
    "        \n",
    "        # Fine-tuned model response (if available)\n",
    "        if evaluator_ft:\n",
    "            ft_response = evaluator_ft.generate_response(input_text)\n",
    "            print(f\"üéØ Fine-tuned: {ft_response}\")\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "\n",
    "# Run enhanced qualitative analysis\n",
    "enhanced_qualitative_analysis(num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643c4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà RESPONSE QUALITY ANALYSIS\n",
      "==================================================\n",
      "üîÑ Loading model with LoRA adapter...\n",
      "‚úÖ Loading adapter from: D:\\Personalized_Chatbot\\models\\fine-tuned-chatbot\\checkpoint-27414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Personalized_Chatbot\\venv\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "d:\\Personalized_Chatbot\\venv\\lib\\site-packages\\peft\\peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_fc.lora_B.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight', 'base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "Analyzing 10 responses...\n",
      "\n",
      "95882. Input: mom working full time well college student...\n",
      "    Response: daniel brough... ‚úÖ Good\n",
      "86693. Input: must good musician make career...\n",
      "    Response: is a great artist... ‚úÖ Good\n",
      "16240. Input: oh cool hate reading...\n",
      "    Response: The Daily Kos... ‚úÖ Good\n",
      "15004. Input: hello name jessica...\n",
      "    Response: My name is Jessica, and I am from the United States.\n",
      "My name... ‚ùå Issues: Too long\n",
      "27564. Input: sound beautiful politics never miss chance vote...\n",
      "    Response: Paul G.\n",
      "Executive Producer: Jon Stewart\n",
      "Producer: Neil Gaima... ‚ùå Issues: Too long\n",
      "44477. Input: animal dorm...\n",
      "    Response: C.J.... ‚ùå Issues: Too short\n",
      "42117. Input: like looking little doll victorian era outfit beau...\n",
      "    Response: don't take... ‚úÖ Good\n",
      "21931. Input: understand know anything truck mike...\n",
      "    Response: know anything truck m... ‚úÖ Good\n",
      "12295. Input: hi texas...\n",
      "    Response: David G.... ‚úÖ Good\n",
      "91815. Input: oh okay long playing basketball...\n",
      "    Response: Don't worry, we're going to win!\n",
      "In the meantime, you can si... ‚ùå Issues: Too long\n",
      "\n",
      "üìä Overall Quality Score: 60.0% (6/10 good responses)\n"
     ]
    }
   ],
   "source": [
    "def response_quality_analysis():\n",
    "    \"\"\"Analyze response quality with scoring\"\"\"\n",
    "    print(\"\\nüìà RESPONSE QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load sample data\n",
    "    try:\n",
    "        df = pd.read_csv(r'D:\\Personalized_Chatbot\\data\\processed\\cleaned_conversations.csv')\n",
    "        samples = df.sample(10)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    adapter_path = None\n",
    "    model_dir = r\"D:\\Personalized_Chatbot\\models\\fine-tuned-chatbot\"\n",
    "    if os.path.exists(model_dir):\n",
    "        checkpoints = []\n",
    "        for item in os.listdir(model_dir):\n",
    "            if item.startswith(\"checkpoint-\"):\n",
    "                try:\n",
    "                    step = int(item.split(\"-\")[1])\n",
    "                    checkpoints.append((step, os.path.join(model_dir, item)))\n",
    "                except:\n",
    "                    continue\n",
    "        if checkpoints:\n",
    "            latest_step, adapter_path = max(checkpoints, key=lambda x: x[0])\n",
    "    \n",
    "    evaluator = FixedChatbotEvaluator(adapter_path=adapter_path)\n",
    "    \n",
    "    # Analyze responses\n",
    "    good_responses = 0\n",
    "    total_responses = len(samples)\n",
    "    \n",
    "    print(f\"Analyzing {total_responses} responses...\\n\")\n",
    "    \n",
    "    for idx, row in samples.iterrows():\n",
    "        input_text = row['input']\n",
    "        reference = row['response']\n",
    "        generated = evaluator.generate_response(input_text)\n",
    "        \n",
    "        # Simple quality checks\n",
    "        quality_issues = []\n",
    "        \n",
    "        if len(generated.strip()) < 5:\n",
    "            quality_issues.append(\"Too short\")\n",
    "        elif len(generated) > 200:\n",
    "            quality_issues.append(\"Too long\")\n",
    "        elif generated.lower().strip() in [\"\", \"i don't know\", \"i don't understand\"]:\n",
    "            quality_issues.append(\"Generic response\")\n",
    "        elif generated == input_text:\n",
    "            quality_issues.append(\"Repeats input\")\n",
    "        else:\n",
    "            good_responses += 1\n",
    "        \n",
    "        status = \"‚úÖ Good\" if not quality_issues else f\"‚ùå Issues: {', '.join(quality_issues)}\"\n",
    "        \n",
    "        print(f\"{idx+1:2d}. Input: {input_text[:50]}...\")\n",
    "        print(f\"    Response: {generated[:60]}... {status}\")\n",
    "    \n",
    "    quality_score = (good_responses / total_responses) * 100\n",
    "    print(f\"\\nüìä Overall Quality Score: {quality_score:.1f}% ({good_responses}/{total_responses} good responses)\")\n",
    "\n",
    "# Run quality analysis\n",
    "response_quality_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fba191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results exported to: evaluation_results_20251013_220844.md\n"
     ]
    }
   ],
   "source": [
    "def export_evaluation_results(results_ft, results_base):\n",
    "    \"\"\"Export evaluation results to a markdown file\"\"\"\n",
    "    import datetime\n",
    "    import math\n",
    "\n",
    "    if not results_ft and not results_base:\n",
    "        print(\"‚ùå No results to export\")\n",
    "        return\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    parts = []\n",
    "    parts.append(f\"# Chatbot Evaluation Results\\n\")\n",
    "    parts.append(f\"**Generated on:** {timestamp}\\n\\n\")\n",
    "    parts.append(\"## üìä Evaluation Metrics\\n\\n\")\n",
    "\n",
    "    if results_base:\n",
    "        parts.append(\"### Base Model Results\\n\")\n",
    "        parts.append(\"| Metric | Value |\\n\")\n",
    "        parts.append(\"|--------|-------|\\n\")\n",
    "        for metric, value in results_base.items():\n",
    "            try:\n",
    "                parts.append(f\"| {metric} | {value:.4f} |\\n\")\n",
    "            except Exception:\n",
    "                parts.append(f\"| {metric} | {value} |\\n\")\n",
    "        parts.append(\"\\n\")\n",
    "\n",
    "    if results_ft:\n",
    "        parts.append(\"### Fine-tuned Model Results\\n\")\n",
    "        parts.append(\"| Metric | Value | Improvement |\\n\")\n",
    "        parts.append(\"|--------|-------|-------------|\\n\")\n",
    "        for metric, value in results_ft.items():\n",
    "            if results_base and metric in results_base:\n",
    "                base_val = results_base.get(metric)\n",
    "                try:\n",
    "                    if base_val in (None, 0):\n",
    "                        improvement_str = \"-\"\n",
    "                    else:\n",
    "                        improvement = ((value - base_val) / base_val) * 100\n",
    "                        improvement_str = f\"{improvement:+.1f}%\"\n",
    "                except Exception:\n",
    "                    improvement_str = \"-\"\n",
    "            else:\n",
    "                improvement_str = \"-\"\n",
    "\n",
    "            try:\n",
    "                parts.append(f\"| {metric} | {value:.4f} | {improvement_str} |\\n\")\n",
    "            except Exception:\n",
    "                parts.append(f\"| {metric} | {value} | {improvement_str} |\\n\")\n",
    "        parts.append(\"\\n\")\n",
    "\n",
    "    parts.append(\"## üìà Summary\\n\\n\")\n",
    "    parts.append(f\"- **Evaluation completed:** {timestamp}\\n\")\n",
    "    parts.append(f\"- **Samples used:** 25\\n\")\n",
    "    parts.append(f\"- **Models compared:** Base vs Fine-tuned\\n\")\n",
    "    parts.append(f\"- **Key metrics:** BLEU, ROUGE-1, ROUGE-2, ROUGE-L, Perplexity\\n\")\n",
    "\n",
    "    content = \"\".join(parts)\n",
    "\n",
    "    # Save to file\n",
    "    filename = f\"evaluation_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"‚úÖ Results exported to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to write results file: {e}\")\n",
    "\n",
    "    return content\n",
    "\n",
    "# Export results\n",
    "if results_ft or results_base:\n",
    "    export_evaluation_results(results_ft, results_base)\n",
    "else:\n",
    "    print(\"‚ùå No results available for export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
